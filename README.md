# ğŸ’® Araliya Tiny-byLLM Plugin

Seamlessly integrates byllm calls with TinyLlama through a local tiny-server, supporting fine-tuning and evaluationÂ workflows.

---

## ğŸš€ Quick Start


### ğŸ”§Tiny-byLLM(Plugin)

**Installation**
```bash
cd tiny-byllm/
pip install -e .
```

### ğŸ–¥ï¸ Run Tiny Server

**Install dependencies:**

```bash
pip install fastapi "uvicorn[standard]" accelerate outlines transformers peft datasets
```

**Run the server:**

```bash
cd tiny-server/
uvicorn app:app --host 0.0.0.0 --port 7000
```
---

## âœ¨ Features

ğŸ”Œ Seamless Jac integration

ğŸ§  TinyLlama 1.1B backbone

ğŸ”„ Intelligent training & evaluation process

ğŸŒ Independent OpenAI APIâ€“compatible tiny-server

ğŸ› Hot-swappable LoRA adapters (as for multitask usage â€” to be added)

   
---

## ğŸ”§ Compatibility

âœ… Updated to work with byllm

ğŸ—‚ Use the mtllm-compatible branch for mtllmÂ (deprecated)

---

## ğŸ—ï¸ Architecture

<p align="center">
  <img src="imgs/pic1.jpeg" alt="System Overview" width="600"/>
</p>

The plugin intercepts byllm calls and, depending on the mode set by the tiny-server, routes inference either to a local TinyLLaMA server (running on port 7000) or to a cloud LLM.

The local TinyLLaMA server exchanges:

- Training Data / Inference results with the plugin.

- Control signals to manage fine-tuning, evaluation, and adapter usage (the available control endpoints are documented in the tiny-server README).

- The tiny-server itself is an OpenAI APIâ€“compatible server that supports constrained decoding through outlines, enabling structured output generation. This architecture allows seamless switching between local and cloud backends while ensuring flexible, reliable integration with Jac.

# ğŸ”„ Adaptive Mode Cycle

<p align="center">
  <img src="imgs/pic2.jpeg" alt="Modes" width="600"/>
</p>

ğŸŒ **Cloud Mode** â€“ The system operates fully on the cloud LLM, both to answer queries and to collect training data for the local model.

ğŸ’¤ **Idle Mode** â€“ Training of the local TinyLLaMA runs as a subprocess, while the cloud LLM continues handling responses.

ğŸ§ª **Eval Mode** â€“ The local modelâ€™s answers are generated and then sent to the cloud LLM for evaluation.

âŒ If the local modelâ€™s answers fail, the system returns to **Cloud Mode** to gather more data and continue training.

âœ… If the local model passes multiple evaluations in a row, the system advances to **Local Mode**.

ğŸ’» **Local Mode** â€“ The local TinyLLaMA serves answers directly. To ensure ongoing quality, it periodically performs random evaluation checks. If a random eval fails, the system reverts to training mode and resumes the cycle to further improve the local model.


