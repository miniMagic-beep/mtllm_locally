# ğŸ’® Araliya Tiny-byLLM Plugin

Seamlessly integrates byllm calls with TinyLlama through a local tiny-server, supporting fine-tuning and evaluationÂ workflows.

[![Araliya Demo Video](http://img.youtube.com/vi/5T9eMUqUlZs/0.jpg)](http://www.youtube.com/watch?v=5T9eMUqUlZs "Watch the Demo")

> **ğŸ¥ Watch the demo:** [Araliya - mtllm plugin for tinyllama](http://www.youtube.com/watch?v=5T9eMUqUlZs)

---

## ğŸš€ Quick Start


### ğŸ”§Tiny-byLLM(Plugin)

**Installation**
```bash
cd tiny-byllm/
pip install -e .
```

### ğŸ–¥ï¸ Run Tiny Server

**Install dependencies:**

```bash
pip install fastapi "uvicorn[standard]" accelerate outlines transformers peft datasets
```

**Run the server:**

```bash
cd tiny-server/
uvicorn app:app --host 0.0.0.0 --port 7000
```
---

## âœ¨ Features

ğŸ”Œ Seamless Jac integration

ğŸ§  TinyLlama 1.1B backbone

ğŸ”„ Intelligent training & evaluation process

ğŸŒ Independent OpenAI APIâ€“compatible tiny-server

ğŸ› Hot-swappable LoRA adapters (as for multitask usage â€” to be added)

   
---

## ğŸ”§ Compatibility

âœ… Updated to work with byllm

ğŸ—‚ Use the mtllm-compatible branch for mtllmÂ (deprecated)

---

## ğŸ—ï¸ System Architecture

<p align="center">
  <img src="imgs/pic1.jpeg" alt="System Overview" width="600"/>
</p>

The plugin intercepts **byllm** calls and, depending on the mode set by the **tiny-server**, routes inference either to:

- ğŸ–¥ï¸ A local **TinyLLaMA server** (running on port 7000)  
- â˜ï¸ A **cloud LLM**

---

### ğŸ”„ Local TinyLLaMA Server Exchanges
- ğŸ“‚ **Exchange Training Data / Inference results** with the plugin  
- âš™ï¸ **Control signals** to manage fine-tuning, evaluation or mode switch
  *(the available control endpoints are documented in the tiny-server README)*

---

### ğŸŒ tiny-server
- An **OpenAI APIâ€“compatible server**  
- Supports **constrained decoding** through *outlines*, enabling structured output generation  
- Allows seamless switching between **local** and **cloud backends**  
- Ensures flexible, reliable integration with **Jac**

## ğŸ”„ Adaptive Mode Cycle

<p align="center">
  <img src="imgs/pic2.jpeg" alt="Modes" width="600"/>
</p>

ğŸŒ **Cloud Mode** â€“ The system operates fully on the cloud LLM, both to answer queries and to collect training data for the local model.

ğŸ’¤ **Idle Mode** â€“ Training of the local TinyLLaMA runs as a subprocess, while the cloud LLM continues handling responses.

ğŸ§ª **Eval Mode** â€“ The local modelâ€™s answers are generated and then sent to the cloud LLM for evaluation.

âŒ If the local modelâ€™s answers fail, the system returns to **Cloud Mode** to gather more data and continue training.

âœ… If the local model passes multiple evaluations in a row, the system advances to **Local Mode**.

ğŸ’» **Local Mode** â€“ The local TinyLLaMA serves answers directly. To ensure ongoing quality, it periodically performs random evaluation checks. If a random eval fails, the system reverts to training mode and resumes the cycle to further improve the local model.


