# âš™ï¸ mtllm_locally

Seamlessly integrates `byllm` calls with **TinyLlama** through a local tiny-server, supporting fine-tuning and evaluation workflows.

### Local LLM Plugin for Jaclang  
A custom MTLLM plugin and local server to integrate a specialized and self-improving **TinyLlama** model into Jaclang's `by llm()` workflow.

---

## âœ¨ Features

- ğŸ”Œ **Seamless Jac integration** â€“ Plug directly into Jaclangâ€™s `by llm()` calls without extra setup.  
- ğŸ§  **TinyLlama 1.1B backbone** â€“ Lightweight, efficient model designed for local execution.  
- ğŸ”„ **Intelligent training & evaluation process** â€“ Collects prompts, fine-tunes locally, and switches to the updated model after evaluation passes.  
- ğŸŒ **Independent tiny-server** â€“ Runs a local `llama.cpp`-powered server with OpenAI APIâ€“compatible endpoints for easy integration.
- ğŸ› **Hot-swappable LoRA adapters** *(planned)* â€“ Enables multitask usage with flexible adapter switching.  

---

## ğŸ”§ Compatibility

- âœ… **Updated for `byllm`** â€“ Fully supports the latest Jaclang LLM plugin system.  
- ğŸ—‚ **Legacy support** â€“ Use the `mtllm-compatible` branch for older MTLLM workflows (deprecated).  

---

## ğŸŸ¢ Current Status

- **ğŸ–¥ï¸ Local LLM Server**: A local server is running using **`llama.cpp`**, serving the base **TinyLlama** model via API endpoint.  
- **ğŸ”Œ Custom MTLLM Plugin**: A Jaclang plugin uses **`LiteLLM`** to intercept `by llm()` calls and route them to the local server.  

---

## âš¡ How It Works

1. Prompts are first routed to a **global model** (Gemini) to generate responses.  
2. Collected data (â‰ˆ100 samples) is used to fine-tune the **TinyLlama** model locally.  
3. After fine-tuning, the local modelâ€™s outputs are **evaluated** against Geminiâ€™s responses.  
4. âœ… If evaluation passes â†’ the system switches to the local TinyLlama for serving requests.  
5. ğŸ”„ This cycle repeats, enabling **continuous self-improvement**.  

---

## â–¶ï¸ Running the Project

1. **Start the local server**  
   ```bash
   ./tinyllama-server --model tinyllama-v1.0.gguf --port 8080
   ```
   
2. **Run Jaclang with the plugin**
   ```bash
   jac run main.jac
   ```

3. **Configure API endpoints** in ```config.json``` to ensure both global (Gemini) and local (TinyLlama server) models are accessible.
