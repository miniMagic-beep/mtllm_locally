{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fae655-8402-4856-aecb-833a60e657dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import outlines\n",
    "from outlines.types import JsonSchema\n",
    "\n",
    "# ---------------------\n",
    "# Load model once\n",
    "# ---------------------\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "try:\n",
    "    model_hf = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "except Exception:\n",
    "    model_hf = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "\n",
    "model = outlines.from_transformers(model_hf, tokenizer)\n",
    "\n",
    "# ---------------------\n",
    "# FastAPI setup\n",
    "# ---------------------\n",
    "app = FastAPI(title=\"TinyLlama Constrained API\")\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class JsonConstrainedRequest(BaseModel):\n",
    "    messages: List[ChatMessage]\n",
    "    json_schema: Dict[str, Any]\n",
    "\n",
    "@app.post(\"/constrained/json\")\n",
    "def constrained_json(req: JsonConstrainedRequest):\n",
    "    try:\n",
    "        # Simple prompt builder\n",
    "        prompt = \"\"\n",
    "        for m in req.messages:\n",
    "            prompt += f\"<|{m.role}|> {m.content}\\n\"\n",
    "        prompt += \"<|assistant|> \"\n",
    "\n",
    "        schema = JsonSchema(req.json_schema)\n",
    "        generator = outlines.generate.json(model, schema)\n",
    "        text = generator(prompt, temperature=0.2)\n",
    "\n",
    "        parsed = json.loads(text)\n",
    "        return {\"content\": parsed, \"raw\": text}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325717f-bee1-43ac-8e63-6f08cafc83b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
